{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f427fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random \n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a51fe409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 'messy_product_reviews.csv' with 100 rows.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def introduce_typo(text):\n",
    "    \"\"\"Randomly swaps characters or drops one to create a typo.\"\"\"\n",
    "    if not isinstance(text, str) or len(text) < 4:\n",
    "        return text\n",
    "    \n",
    "    if random.random() > 0.3: # Only mess up 30% of text\n",
    "        return text\n",
    "\n",
    "    chars = list(text)\n",
    "    idx = random.randint(0, len(chars) - 2)\n",
    "    action = random.choice(['swap', 'drop', 'duplicate'])\n",
    "    \n",
    "    if action == 'swap':\n",
    "        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]\n",
    "    elif action == 'drop':\n",
    "        chars.pop(idx)\n",
    "    elif action == 'duplicate':\n",
    "        chars.insert(idx, chars[idx])\n",
    "        \n",
    "    return \"\".join(chars)\n",
    "\n",
    "def generate_messy_data(num_rows=100):\n",
    "    products = ['Wireless Mouse', 'Gaming Monitor', 'Mechanical Keyboard', 'HDMI Cable', 'Webcam']\n",
    "    names = ['John Doe', 'Jane Smith', 'Bob Wilson', 'alice walker', 'charlie brown', None] # Note lowercase and None\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for i in range(1, num_rows + 1):\n",
    "        # 1. Generate Basic Fields\n",
    "        row_id = i if random.random() > 0.05 else np.nan  # 5% missing IDs\n",
    "        name = random.choice(names)\n",
    "        product = random.choice(products)\n",
    "        \n",
    "        # 2. Messy Ratings (Mix of int, str, out of bounds, and missing)\n",
    "        rand_val = random.random()\n",
    "        if rand_val < 0.1: rating = np.nan\n",
    "        elif rand_val < 0.2: rating = \"Five\"\n",
    "        elif rand_val < 0.3: rating = 100 # Logic error\n",
    "        else: rating = random.randint(1, 5)\n",
    "        \n",
    "        # 3. Messy Dates (Different formats, invalid dates)\n",
    "        rand_date = random.random()\n",
    "        base_date = datetime.date(2023, 1, 1) + datetime.timedelta(days=random.randint(0, 365))\n",
    "        if rand_date < 0.1:\n",
    "            date_str = None\n",
    "        elif rand_date < 0.2:\n",
    "            date_str = \"2023/13/01\" # Invalid month\n",
    "        elif rand_date < 0.3:\n",
    "            date_str = base_date.strftime(\"%d-%m-%Y\") # European format\n",
    "        else:\n",
    "            date_str = base_date.strftime(\"%Y-%m-%d\") # ISO format\n",
    "\n",
    "        # 4. Text with Typos\n",
    "        reviews = [\n",
    "            \"Great product works well.\",\n",
    "            \"Terrible, broke after one day.\",\n",
    "            \"Fast shipping, highly recommended.\",\n",
    "            \"Not what I expected, color is wrong.\",\n",
    "            \"Okay for the price.\"\n",
    "        ]\n",
    "        review_text = introduce_typo(random.choice(reviews))\n",
    "        \n",
    "        # 5. Inconsistent Capitalization in Category\n",
    "        category = random.choice(['Electronics', 'electronics', 'ELECTRONICS', np.nan])\n",
    "\n",
    "        data.append([row_id, name, product, category, rating, date_str, review_text])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=['review_id', 'customer_name', 'product_name', 'category', 'rating', 'date', 'review_text'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('messy_product_reviews.csv', index=False)\n",
    "    print(f\"Successfully generated 'messy_product_reviews.csv' with {num_rows} rows.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_messy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4415713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Cleaning Pipeline ---\n",
      "Loaded 100 rows.\n",
      "Dropped 2 rows with missing IDs.\n",
      "\n",
      "--- Data Quality Report ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 98 entries, 0 to 99\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   review_id      98 non-null     int64         \n",
      " 1   customer_name  98 non-null     object        \n",
      " 2   product_name   98 non-null     object        \n",
      " 3   category       98 non-null     object        \n",
      " 4   rating         98 non-null     int64         \n",
      " 5   date           98 non-null     datetime64[ns]\n",
      " 6   review_text    98 non-null     object        \n",
      "dtypes: datetime64[ns](1), int64(2), object(4)\n",
      "memory usage: 6.1+ KB\n",
      "None\n",
      "\n",
      "Sample Data:\n",
      "   review_id customer_name    product_name     category  rating       date  \\\n",
      "0          1      John Doe      HDMI Cable  Electronics       1 2026-02-01   \n",
      "1          2    Bob Wilson  Wireless Mouse  Electronics       3 2023-12-28   \n",
      "2          3     Anonymous      HDMI Cable  Electronics       5 2026-02-01   \n",
      "3          4    Jane Smith      HDMI Cable  Electronics       2 2023-05-02   \n",
      "4          5      John Doe          Webcam  Electronics       3 2026-02-01   \n",
      "\n",
      "                            review_text  \n",
      "0             Great product works well.  \n",
      "1       Terrible, broke after one daay.  \n",
      "2                   Okay for the price.  \n",
      "3  Not what I expceted, color is wrong.  \n",
      "4             Great product wokrs well.  \n",
      "\n",
      "Pipeline Complete. Clean data saved to 'clean_product_reviews.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44153/4081271205.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "def clean_data(input_file, output_file):\n",
    "    print(\"--- Starting Data Cleaning Pipeline ---\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    df = pd.read_csv(input_file)\n",
    "    initial_count = len(df)\n",
    "    print(f\"Loaded {initial_count} rows.\")\n",
    "\n",
    "    # 2. Handling Primary Keys (review_id)\n",
    "    # Strategy: Drop rows where ID is missing. In production, we might log these to an error table.\n",
    "    df.dropna(subset=['review_id'], inplace=True)\n",
    "    df['review_id'] = df['review_id'].astype(int) # Ensure it's an integer\n",
    "    print(f\"Dropped {initial_count - len(df)} rows with missing IDs.\")\n",
    "\n",
    "    # 3. Standardizing Text Columns (String Manipulation)\n",
    "    # Fix Category: standardize to Title Case (e.g., 'electronics' -> 'Electronics')\n",
    "    df['category'] = df['category'].fillna('Unknown').str.title().str.strip()\n",
    "    \n",
    "    # Fix Name: Title case and handle missing\n",
    "    df['customer_name'] = df['customer_name'].fillna('Anonymous').str.title().str.strip()\n",
    "\n",
    "    # 4. Cleaning Numerical Data (Ratings)\n",
    "    # Issue: We have \"Five\", 100, and NaNs.\n",
    "    \n",
    "    def clean_rating(val):\n",
    "        if pd.isna(val):\n",
    "            return 0 # Strategy: Default missing ratings to 0\n",
    "        \n",
    "        # Try to convert string numbers \"Five\" -> 5 (Simple mapping for this example)\n",
    "        str_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "        if isinstance(val, str) and val.lower() in str_map:\n",
    "            return str_map[val.lower()]\n",
    "            \n",
    "        try:\n",
    "            # Force numeric\n",
    "            num = float(val)\n",
    "            # Strategy: Clamp values between 1 and 5\n",
    "            if num > 5: return 5\n",
    "            if num < 1: return 1\n",
    "            return int(num)\n",
    "        except ValueError:\n",
    "            return 0 # If it's garbage text, set to 0\n",
    "\n",
    "    df['rating'] = df['rating'].apply(clean_rating)\n",
    "\n",
    "    # 5. Cleaning Dates\n",
    "    # Strategy: Coerce errors. If a date is \"2023/13/01\", it becomes NaT (Not a Time), then we handle NaT.\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    # Fill missing dates with today's date or a default\n",
    "    df['date'] = df['date'].fillna(pd.Timestamp.today().normalize())\n",
    "\n",
    "    # 6. Cleaning Review Text\n",
    "    # Strategy: Remove leading/trailing whitespace, fill NaNs\n",
    "    df['review_text'] = df['review_text'].fillna(\"No review text provided.\")\n",
    "    df['review_text'] = df['review_text'].str.strip()\n",
    "\n",
    "    # 7. Deduplication\n",
    "    # Ensure no duplicate review_ids exist\n",
    "    before_dedup = len(df)\n",
    "    df.drop_duplicates(subset=['review_id'], inplace=True)\n",
    "    if len(df) < before_dedup:\n",
    "        print(f\"Removed {before_dedup - len(df)} duplicate records.\")\n",
    "\n",
    "    # 8. Final Inspection\n",
    "    print(\"\\n--- Data Quality Report ---\")\n",
    "    print(df.info())\n",
    "    print(\"\\nSample Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 9. Export Clean Data\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPipeline Complete. Clean data saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_data('messy_product_reviews.csv', 'clean_product_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24146d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ETL & Enrichment Pipeline ---\n",
      "Loaded 100 raw rows.\n",
      "Running Sentiment Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44153/1852234646.py:52: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Data Preview ---\n",
      "                            review_text  rating  sentiment_score  \\\n",
      "0             Great product works well.       1              0.8   \n",
      "1       Terrible, broke after one daay.       3             -1.0   \n",
      "2                   Okay for the price.       5              0.5   \n",
      "3  Not what I expceted, color is wrong.       2             -0.5   \n",
      "4             Great product wokrs well.       3              0.8   \n",
      "\n",
      "  sentiment_category  \n",
      "0           Positive  \n",
      "1           Negative  \n",
      "2           Positive  \n",
      "3           Negative  \n",
      "4           Positive  \n",
      "\n",
      "Pipeline Complete. Enriched data saved to 'enriched_reviews.csv'.\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Returns a polarity score between -1.0 (Negative) and 1.0 (Positive).\n",
    "    0.0 is Neutral.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def get_sentiment_label(score):\n",
    "    \"\"\"\n",
    "    Categorizes the numeric score into a business-friendly label.\n",
    "    \"\"\"\n",
    "    if score > 0.1:\n",
    "        return 'Positive'\n",
    "    elif score < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def process_reviews_pipeline(input_file, output_file):\n",
    "    print(\"--- Starting ETL & Enrichment Pipeline ---\")\n",
    "    \n",
    "    # 1. LOAD\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df)} raw rows.\")\n",
    "\n",
    "    # 2. CLEAN (The steps we established previously)\n",
    "    df.dropna(subset=['review_id'], inplace=True)\n",
    "    df['review_id'] = df['review_id'].astype(int)\n",
    "    \n",
    "    # Fix strings\n",
    "    df['category'] = df['category'].fillna('Unknown').str.title().str.strip()\n",
    "    df['customer_name'] = df['customer_name'].fillna('Anonymous').str.title().str.strip()\n",
    "    \n",
    "    # Fix ratings\n",
    "    def clean_rating(val):\n",
    "        if pd.isna(val): return 0\n",
    "        str_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "        if isinstance(val, str) and val.lower() in str_map:\n",
    "            return str_map[val.lower()]\n",
    "        try:\n",
    "            num = float(val)\n",
    "            if num > 5: return 5\n",
    "            if num < 1: return 1\n",
    "            return int(num)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    df['rating'] = df['rating'].apply(clean_rating)\n",
    "    \n",
    "    # Fix dates\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['date'] = df['date'].fillna(pd.Timestamp.today().normalize())\n",
    "    \n",
    "    # Fix text\n",
    "    df['review_text'] = df['review_text'].fillna(\"No review text.\")\n",
    "    df['review_text'] = df['review_text'].str.strip()\n",
    "    \n",
    "    # Deduplicate\n",
    "    df.drop_duplicates(subset=['review_id'], inplace=True)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. ENRICH (Sentiment Analysis)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Running Sentiment Analysis...\")\n",
    "    \n",
    "    # Apply the sentiment function to the clean text column\n",
    "    df['sentiment_score'] = df['review_text'].apply(get_sentiment_score)\n",
    "    \n",
    "    # Create a label for easier dashboarding (PowerBI/Tableau love this)\n",
    "    df['sentiment_category'] = df['sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "    # 4. EXPORT\n",
    "    print(\"\\n--- Final Data Preview ---\")\n",
    "    # Show specific columns to verify sentiment worked\n",
    "    print(df[['review_text', 'rating', 'sentiment_score', 'sentiment_category']].head(5))\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPipeline Complete. Enriched data saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure you generate the messy data first using the previous script!\n",
    "    try:\n",
    "        process_reviews_pipeline('messy_product_reviews.csv', 'enriched_reviews.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'messy_product_reviews.csv' not found. Please run the generation script first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22240e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting ETL & Enrichment Pipeline (VADER Edition) ---\n",
      "Loaded 100 raw rows.\n",
      "Running VADER Sentiment Analysis...\n",
      "\n",
      "--- Final Data Preview ---\n",
      "                            review_text  rating  sentiment_score  \\\n",
      "0             Great product works well.       1           0.7351   \n",
      "1       Terrible, broke after one daay.       3          -0.7096   \n",
      "2                   Okay for the price.       5           0.2263   \n",
      "3  Not what I expceted, color is wrong.       2          -0.4767   \n",
      "4             Great product wokrs well.       3           0.7351   \n",
      "\n",
      "  sentiment_category  \n",
      "0           Positive  \n",
      "1           Negative  \n",
      "2           Positive  \n",
      "3           Negative  \n",
      "4           Positive  \n",
      "\n",
      "Pipeline Complete. Enriched data saved to 'enriched_reviews_vader.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44153/940135207.py:71: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# --- SETUP: VADER INITIALIZATION ---\n",
    "# A robust pipeline ensures dependencies exist before running\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except LookupError:\n",
    "    print(\"Downloading VADER lexicon for the first time...\")\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the analyzer once (Global scope is more efficient than initializing per row)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Uses VADER to return a 'compound' score between -1.0 (Negative) and 1.0 (Positive).\n",
    "    VADER is smarter than TextBlob: it understands \"BAD!!!\" vs \"bad\".\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0.0\n",
    "    \n",
    "    # sia.polarity_scores returns dict: {'neg': 0.0, 'neu': 0.5, 'pos': 0.5, 'compound': 0.0}\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "def get_sentiment_label(score):\n",
    "    \"\"\"\n",
    "    Categorizes the numeric score into a business-friendly label.\n",
    "    VADER standard thresholds are usually +/- 0.05, but we will use 0.1 \n",
    "    to be safe and avoid labeling weak sentences as strong opinions.\n",
    "    \"\"\"\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def process_reviews_pipeline(input_file, output_file):\n",
    "    print(\"--- Starting ETL & Enrichment Pipeline (VADER Edition) ---\")\n",
    "    \n",
    "    # 1. LOAD\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"Loaded {len(df)} raw rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file '{input_file}' not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. CLEAN (Standard Logic)\n",
    "    df.dropna(subset=['review_id'], inplace=True)\n",
    "    df['review_id'] = df['review_id'].astype(int)\n",
    "    \n",
    "    # Fix strings\n",
    "    df['category'] = df['category'].fillna('Unknown').str.title().str.strip()\n",
    "    df['customer_name'] = df['customer_name'].fillna('Anonymous').str.title().str.strip()\n",
    "    \n",
    "    # Fix ratings\n",
    "    def clean_rating(val):\n",
    "        if pd.isna(val): return 0\n",
    "        str_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5}\n",
    "        if isinstance(val, str) and val.lower() in str_map:\n",
    "            return str_map[val.lower()]\n",
    "        try:\n",
    "            num = float(val)\n",
    "            if num > 5: return 5\n",
    "            if num < 1: return 1\n",
    "            return int(num)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    df['rating'] = df['rating'].apply(clean_rating)\n",
    "    \n",
    "    # Fix dates\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['date'] = df['date'].fillna(pd.Timestamp.today().normalize())\n",
    "    \n",
    "    # Fix text\n",
    "    df['review_text'] = df['review_text'].fillna(\"No review text.\")\n",
    "    df['review_text'] = df['review_text'].str.strip()\n",
    "    \n",
    "    # Deduplicate\n",
    "    df.drop_duplicates(subset=['review_id'], inplace=True)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3. ENRICH (Sentiment Analysis with VADER)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"Running VADER Sentiment Analysis...\")\n",
    "    \n",
    "    # Apply the VADER function\n",
    "    df['sentiment_score'] = df['review_text'].apply(get_sentiment_score)\n",
    "    \n",
    "    # Create labels\n",
    "    df['sentiment_category'] = df['sentiment_score'].apply(get_sentiment_label)\n",
    "\n",
    "    # 4. EXPORT\n",
    "    print(\"\\n--- Final Data Preview ---\")\n",
    "    # Show text and scores to verify VADER is working\n",
    "    print(df[['review_text', 'rating', 'sentiment_score', 'sentiment_category']].head(5))\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPipeline Complete. Enriched data saved to '{output_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_reviews_pipeline('messy_product_reviews.csv', 'enriched_reviews_vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a65c9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Complaint Analysis ---\n",
      "Identified 69 negative reviews out of 98 total.\n",
      "\n",
      "[Metric 1] Top 3 Products with Most Complaints:\n",
      "product_name\n",
      "Mechanical Keyboard    18\n",
      "Webcam                 15\n",
      "Wireless Mouse         14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Metric 2] Top 3 Recurring Complaint Phrases:\n",
      " - 'terrible broke' (appeared 20 times)\n",
      " - 'broke day' (appeared 19 times)\n",
      " - 'color wrong' (appeared 17 times)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def analyze_complaints(input_file):\n",
    "    print(\"--- Starting Complaint Analysis ---\")\n",
    "    \n",
    "    # 1. Load the Enriched Data\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: File not found. Run the cleaning script first.\")\n",
    "        return\n",
    "\n",
    "    # 2. Filter: Isolate the \"Problem\" Rows\n",
    "    # We define a complaint as Sentiment = Negative OR Rating <= 2\n",
    "    complaints_df = df[\n",
    "        (df['sentiment_category'] == 'Negative') | \n",
    "        (df['rating'] <= 2)\n",
    "    ].copy()\n",
    "\n",
    "    if len(complaints_df) == 0:\n",
    "        print(\"Good news! No complaints found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Identified {len(complaints_df)} negative reviews out of {len(df)} total.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Insight A: The \"Who\" (Which products act up?)\n",
    "    # ---------------------------------------------------------\n",
    "    print(\"\\n[Metric 1] Top 3 Products with Most Complaints:\")\n",
    "    top_bad_products = complaints_df['product_name'].value_counts().head(3)\n",
    "    print(top_bad_products)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Insight B: The \"Why\" (Common phrases in text)\n",
    "    # ---------------------------------------------------------\n",
    "    # We use CountVectorizer to find frequent 2-word phrases (bi-grams)\n",
    "    # stop_words='english' removes junk like \"the\", \"and\", \"is\"\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "    \n",
    "    try:\n",
    "        # Fit the vectorizer to the text\n",
    "        bag_of_words = vec.fit_transform(complaints_df['review_text'])\n",
    "        \n",
    "        # Sum the occurrences of each phrase\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        \n",
    "        # Map indices back to words\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "        \n",
    "        # Sort by frequency (Desc)\n",
    "        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\n[Metric 2] Top 3 Recurring Complaint Phrases:\")\n",
    "        for phrase, count in words_freq[:3]:\n",
    "            print(f\" - '{phrase}' (appeared {count} times)\")\n",
    "            \n",
    "    except ValueError:\n",
    "        print(\"\\n[Notice] Not enough text data to form common phrases.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_complaints('enriched_reviews_vader.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
